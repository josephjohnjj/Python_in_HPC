{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9524305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f6e812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578f479",
   "metadata": {},
   "source": [
    "##### In a normal CUDA workflow we have to allocate the memory on the GPU and the move the data to the GPU memory. In CuPy this is not required, the memory allocation and data movement can be done in a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba3a0494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 441 µs, sys: 335 µs, total: 776 µs\n",
      "Wall time: 532 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_gpu_0 = cp.asarray(x_cpu)  # move the ndarray from host mem to GPU0 memeory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e7106",
   "metadata": {},
   "source": [
    "#### In the past any communication between two GPUs had to go throgh the PCIe card. But now NVIDIA offeres a technology called NVLink. NVLink is a direct GPU-to-GPU interconnect that scales multi-GPU input/output (IO) within a node. This makes GPU-to-GPU transfer (D2D tranfer) much faster than GPU-to-Host (D2H transfer) or Host-to-GPU transfer (H2D transfer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7673b300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 224 µs, sys: 169 µs, total: 393 µs\n",
      "Wall time: 321 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with cp.cuda.Device(1):\n",
    "    x_gpu_1 = cp.asarray(x_gpu_0)  # move the ndarray to GPU0 to GPU1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1c166",
   "metadata": {},
   "source": [
    "##### There are two ways to fetch the data from the GPU to host ``cupy.ndarray.get()`` or ``cupy.asnumpy``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75f4369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with cp.cuda.Device(0):\n",
    "    x_cpu = cp.asnumpy(x_gpu_0)  # move the array back to the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36eea697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14eed2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with cp.cuda.Device(1):\n",
    "    x_cpu = x_gpu_1.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "348b267d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5122959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
